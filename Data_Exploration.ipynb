{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This notebook largely serves to allow me to sift through all of the chargemasters and metadata generated via the work already done in [this wonderful repo](https://github.com/vsoch/hospital-chargemaster) (from which I forked my repo). \n",
    "\n",
    "Based upon the originating repo's own README, there's at least some data collection that still needs to be done (e.g. [data from hospitalpriceindex.com](https://search.hospitalpriceindex.com/hospital/Barnes-Jewish-Hospital/5359?page=1) has to be scraped but they're denying IP addresses that try to do so). However, if any of that gets done in here, it won't be until after I've sifted through the current material to make sure I have a handle on what data are already available. \n",
    "\n",
    "It's fairly plausible that I'll then attempt to combine it all into a single sqlite or MongoDB database that can subsequently be analyzed. But I'm getting ahead of myself - first to figure out what we have to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "*Assume everything in this cell is quoted directly from the originating repo README, albeit with some extra content removed for the purposes of streamlining. Anything in italics like this should be assumed to be editorial additions by me.*\n",
    "\n",
    "**From the original README:**\n",
    "\n",
    "## Get List of Hospital Pages\n",
    "We have compiled a list of hospitals and links in the [hospitals.tsv](hospitals.tsv) \n",
    "file, generated via the [0.get_hospitals.py](0.get_hospitals.py) script *which pulls these data from [a Quartz article](https://qz.com/1518545/price-lists-for-the-115-biggest-us-hospitals-new-transparency-law/) detailing ~115 hospital URLs from which the authors were able to find chargemasters in one form or another*. \n",
    "\n",
    "The file includes the following variables, separated by tabs:\n",
    "\n",
    " - **hospital_name** is the human friendly name\n",
    " - **hospital_url** is the human friendly URL, typically the page that includes a link to the data.\n",
    " - **hospital_id** is the unique identifier for the hospital, the hospital name, in lowercase, with spaces replaced with `-`\n",
    "   \n",
    "## Organize Data\n",
    "\n",
    "Each hospital has records kept in a subfolder in the [data](data) folder. Specifically,\n",
    "each subfolder is named according to the hospital name (made all lowercase, with spaces \n",
    "replaced with `-`). If a subfolder begins with an underscore, it means that I wasn't\n",
    "able to find the charge list on the hospital site (and maybe you can help?) \n",
    "Within that folder, you will find:\n",
    "\n",
    " - `scrape.py`: A script to scrape the data\n",
    " - `browser.py`: If we need to interact with a browser, we use selenium to do this.\n",
    " - `latest`: a folder with the last scraped (latest data files)\n",
    " - `YYYY-MM-DD` folders, where each folder includes:\n",
    "   - `records.json` the complete list of records scraped for a particular data\n",
    "   - `*.csv` or `*.xlsx` or `*.json`: the scraped data files.\n",
    "\n",
    "## Parsing\n",
    "This is likely one of the hardest steps. I wanted to see the extent to which I could\n",
    "create a simple parser that would generate a single TSV (tab separted value) file\n",
    "per hospital, with minimally an identifier for a charge, and a price in dollars. If\n",
    "provided, I would also include a description and code:\n",
    "\n",
    " - **charge_code**\n",
    " - **price**\n",
    " - **description**\n",
    " - **hospital_id**\n",
    " - **filename**\n",
    "\n",
    "Each of these parsers is also in the hospital subfolder, and named as \"parser.py.\" The parser would output a data-latest.tsv file at the top level of the folder, along with a dated (by year `data-<year>.tsv`). At some point\n",
    "I realized that there were different kinds of charges, including inpatient, outpatient, DRG (diagnostic related group) and others called\n",
    "\"standard\" or \"average.\" I then went back and added an additional column\n",
    "to the data:\n",
    "\n",
    " - **charge_type** can be one of standard, average, inpatient, outpatient, drg, or (if more detail is supplied) insured, uninsured, pharmacy, or supply. This is not a gold standard labeling but a best effort. If not specified, I labeled as standard, because this would be a good assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "OK, I think I have a handle on this, let's take a look at the data, starting with the hospital metadata.\n",
    "\n",
    "It may be a bit confusing for anyone following along at home, but note that I had already started my own effort to download a bunch of these chargemasters in a far more manual approach than what @vsoch did with `BeautifulSoup` and all. As a result, I may be comparing her dataset at times in this notebook to the one I was developing. Mine was never going to have automated updates like hers however, hence why I'm deferring to her repo and data over my own (while mine may be a bit more comprehensive in terms of number of hospitals, I'd rather the data be up to date as much as possible). \n",
    "\n",
    "That said, as of this writing, I have approximately 600+ unique hospitals included in my chargemaster index, so I'm going to keep an eye out during my EDA to see if my more manual approach may still be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:38:52.567120Z",
     "start_time": "2019-04-21T13:38:52.487935Z"
    }
   },
   "outputs": [],
   "source": [
    "#Make sure any changes to custom packages can be reflected immediately \n",
    "#in the notebook without kernel restart\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:39:03.434382Z",
     "start_time": "2019-04-21T13:38:52.570228Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import the hospital metadata\n",
    "\n",
    "import pandas as pd\n",
    "metadata = pd.read_csv('hospitals.tsv', delimiter = r'\\t')\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T13:39:03.483301Z",
     "start_time": "2019-04-21T13:39:03.437319Z"
    }
   },
   "outputs": [],
   "source": [
    "#Do these data include the huge CA chargemaster dataset on oshpd.ca.gov?\n",
    "metadata[metadata['hospital_url'].str.contains('oshpd.ca.gov')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interesting, it looks like this dataset may not include the full OSHPD chargemaster list.* I find that unlikely however, as @vsoch makes it clear in another part of her README that she built the `oshpd-ca` scraper and found 795+ hospitals in that dataset. **This suggests that this metadata file may not be complete, as it's really just an inventory of the links from the aforementioned Quartz article, and not necessarily a full accounting of the hospitals contained in the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Tabulated Data\n",
    "\n",
    "OK, so the metadata table wasn't super useful (investigating `data/oshpd-ca` confirmed that indeed there are far more files in this repo than hospitals listed in `hospitals.tsv`), but there are **a lot** of files to plow through here! And vsoch was kind enough to try and compile them whenever appropriate in the various hospital/site-specific folders within `data` as `data-latest[-n].tsv` (`-n` indicates that, if the file gets above 100 MB, it's split into `data-latest-1.tsv`, `data-latest-2.tsv`, etc. to avoid going over the GitHub per-file size limit).\n",
    "\n",
    "Let's try to parse all of these TSV files into a single coherent DataFrame! The entire `data` folders is less than 4 GB, and I'm confident that more than half of that is individual XLSX/CSV files, so I think this should be something we can hold in memory easily enough.\n",
    "\n",
    "...still, we'll use some tricks (e.g. making the sub-dataframes as a generator instead of a list) to ensure optimal memory usage, just to be safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:43:02.489999Z",
     "start_time": "2019-04-17T02:43:02.410566Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search through the data/hospital-id folders for data-latest[-n].tsv files\n",
    "# so you can concatenate them into a single DataFrame\n",
    "from glob import glob, iglob\n",
    "\n",
    "for name in iglob('data/*/data-latest*.tsv'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T20:40:16.233282Z",
     "start_time": "2019-04-18T20:39:05.673060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup the full dataframe using iterators/generators to save on memory\n",
    "all_files = iglob('data/*/data-latest*.tsv')\n",
    "individual_dfs = (pd.read_csv(f, delimiter = '\\t', \n",
    "                              low_memory = False,\n",
    "                             thousands = ',') for f in all_files)\n",
    "\n",
    "df = pd.concat(individual_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T20:40:58.253195Z",
     "start_time": "2019-04-18T20:40:16.737287Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info(memory_usage = 'deep', verbose = True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:47:09.548208Z",
     "start_time": "2019-04-17T02:47:09.491273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not bad! At first I was a bit worried when I looked at this initial file in Excel, but Excel improperly interprets the thousands comma on prices as a new field delimiter.** This might work after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T02:52:23.479047Z",
     "start_time": "2019-04-17T02:52:23.432792Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-17T03:00:05.809494Z",
     "start_time": "2019-04-17T02:59:13.047840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Push out unoptimized CSV copy of dataframe so we don't need it to perpetually be in memory\n",
    "\n",
    "df.to_csv('data/all_hospitals-latest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the DataFrame\n",
    "\n",
    "This dataframe is quite large (almost 4GB in memory!) and that's likely to cause all sorts of problems when it comes time to do analysis. So we'll need to make sure we understand the nature of each column's data and then optimize for it (after potentially correcting for things like strings that are actually numbers).\n",
    "\n",
    "To start with, let's check out the nature of the data in each column:\n",
    "\n",
    "1. How often do charge codes exist?\n",
    "2. We'll check, but likely that description exists for most\n",
    "3. Other than decimal numbers, what values are in price?\n",
    "4. What are all of the different charge type values, and which ones can we filter out (e.g. drg)?\n",
    "    * Do charge codes get embedded in descriptions a lot, like what we see in df.tail()? Or is this something that is only present for non-standard charge_type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:40:31.015758Z",
     "start_time": "2019-04-23T03:39:54.010039Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emigre459/anaconda3/envs/HospitalPricing/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/emigre459/anaconda3/envs/HospitalPricing/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# In case you need to reinitialize df after experimenting...\n",
    "\n",
    "#Make sure any changes to custom packages can be reflected immediately \n",
    "#in the notebook without kernel restart\n",
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/all_hospitals-latest.csv', index_col = 0, thousands = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:40:31.107976Z",
     "start_time": "2019-04-23T03:40:31.017974Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>charge_code</th>\n",
       "      <th>price</th>\n",
       "      <th>description</th>\n",
       "      <th>hospital_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>charge_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1980</td>\n",
       "      <td>Adult semi-private (up to two patients) room a...</td>\n",
       "      <td>university-of-virginia-medical-center</td>\n",
       "      <td>university-of-virginia-medical-center.csv</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1980</td>\n",
       "      <td>Adult room and care in the Transitional Care H...</td>\n",
       "      <td>university-of-virginia-medical-center</td>\n",
       "      <td>university-of-virginia-medical-center.csv</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2310</td>\n",
       "      <td>Adult room and care on the Labor &amp; Delivery unit</td>\n",
       "      <td>university-of-virginia-medical-center</td>\n",
       "      <td>university-of-virginia-medical-center.csv</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1980</td>\n",
       "      <td>Pediatric room and care</td>\n",
       "      <td>university-of-virginia-medical-center</td>\n",
       "      <td>university-of-virginia-medical-center.csv</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2448</td>\n",
       "      <td>Psychiatric room and care</td>\n",
       "      <td>university-of-virginia-medical-center</td>\n",
       "      <td>university-of-virginia-medical-center.csv</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  charge_code price                                        description  \\\n",
       "0         NaN  1980  Adult semi-private (up to two patients) room a...   \n",
       "1         NaN  1980  Adult room and care in the Transitional Care H...   \n",
       "2         NaN  2310   Adult room and care on the Labor & Delivery unit   \n",
       "3         NaN  1980                            Pediatric room and care   \n",
       "4         NaN  2448                          Psychiatric room and care   \n",
       "\n",
       "                             hospital_id  \\\n",
       "0  university-of-virginia-medical-center   \n",
       "1  university-of-virginia-medical-center   \n",
       "2  university-of-virginia-medical-center   \n",
       "3  university-of-virginia-medical-center   \n",
       "4  university-of-virginia-medical-center   \n",
       "\n",
       "                                    filename charge_type  \n",
       "0  university-of-virginia-medical-center.csv    standard  \n",
       "1  university-of-virginia-medical-center.csv    standard  \n",
       "2  university-of-virginia-medical-center.csv    standard  \n",
       "3  university-of-virginia-medical-center.csv    standard  \n",
       "4  university-of-virginia-medical-center.csv    standard  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:41:24.514119Z",
     "start_time": "2019-04-23T03:40:31.111894Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of unique values</th>\n",
       "      <th>fraction of all non-null values</th>\n",
       "      <th>make categorical?</th>\n",
       "      <th>data type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>charge_code</th>\n",
       "      <td>1878137</td>\n",
       "      <td>0.29</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>783989</td>\n",
       "      <td>0.12</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>2477488</td>\n",
       "      <td>0.38</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospital_id</th>\n",
       "      <td>379</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charge_type</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             number of unique values  fraction of all non-null values  \\\n",
       "charge_code                  1878137                             0.29   \n",
       "price                         783989                             0.12   \n",
       "description                  2477488                             0.38   \n",
       "hospital_id                      379                             0.00   \n",
       "filename                         705                             0.00   \n",
       "charge_type                       12                             0.00   \n",
       "\n",
       "             make categorical? data type  \n",
       "charge_code               True    object  \n",
       "price                     True    object  \n",
       "description               True    object  \n",
       "hospital_id               True    object  \n",
       "filename                  True    object  \n",
       "charge_type               True    object  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's get a better handle on unique values, so we can figure out what fields make\n",
    "# sense as categoricals and what ones definitely don't\n",
    "\n",
    "unique_vals = pd.DataFrame(df.nunique()).rename(columns = {0: 'number of unique values'})\n",
    "unique_vals['fraction of all non-null values'] = round(unique_vals['number of unique values'] / len(df.dropna()), 2)\n",
    "unique_vals['make categorical?'] = unique_vals['fraction of all non-null values'] < 0.5\n",
    "unique_vals['data type'] = df.dtypes\n",
    "unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:41:28.215298Z",
     "start_time": "2019-04-23T03:41:24.517844Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of unique values</th>\n",
       "      <th>fraction of all non-null values</th>\n",
       "      <th>make categorical?</th>\n",
       "      <th>data type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>charge_code</th>\n",
       "      <td>1878137</td>\n",
       "      <td>0.29</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>783989</td>\n",
       "      <td>0.12</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>2477488</td>\n",
       "      <td>0.38</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospital_id</th>\n",
       "      <td>379</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>705</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charge_type</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             number of unique values  fraction of all non-null values  \\\n",
       "charge_code                  1878137                             0.29   \n",
       "price                         783989                             0.12   \n",
       "description                  2477488                             0.38   \n",
       "hospital_id                      379                             0.00   \n",
       "filename                         705                             0.00   \n",
       "charge_type                       12                             0.00   \n",
       "\n",
       "             make categorical? data type  \n",
       "charge_code               True    object  \n",
       "price                     True    object  \n",
       "description               True    object  \n",
       "hospital_id               True    object  \n",
       "filename                  True    object  \n",
       "charge_type               True    object  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_vals['fraction of all non-null values'] = round(unique_vals['number of unique values'] / len(df.dropna()), 2)\n",
    "unique_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting. There are so many records with repeat information that we can change the dtype of pretty much all of them to be categorical.** Here's what we're going to do:\n",
    "\n",
    "1. `charge_code`, `price`, and `description`: I'm not going to convert these to categoricals\n",
    "    * `charge_code` and `description`: while from a data perspective these would likely be better handled in memory by making anything categorical that has so few unique values that the count of them is less than 50% of the count of all non-null rows, it doesn't make sense to make these fields categoricals, as that implies they are drawing from a common data reference. **That's simply not the case.** \n",
    "        * Given that two different hospitals can have the charge code `1001` refer to two totally different procedures/consumables, there's no reason to add confusion by treating these in the dataset like they have the same meaning. \n",
    "        * The same logic goes for the description field (although that one has me scratching my head a bit, as I'd expect it to be a bit more free text in nature and thus not likely to have repeated values)\n",
    "    * `price`: this should be a continuous variable, not a categorical one!\n",
    "2. `hospital_id`, `filename`, and `charge_type`: these are classic examples of categorical variables and we should convert them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorically Annoying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:41:34.299002Z",
     "start_time": "2019-04-23T03:41:28.217866Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['drg', 'inpatient', 'insurance', 'insured-inpatient',\n",
       "       'insured-outpatient', 'outpatient', 'pharmacy', 'standard', 'supply',\n",
       "       'uninsured', 'uninsured-inpatient', 'uninsured-outpatient'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make categorical columns where appropriate\n",
    "cat_cols = ['hospital_id', 'filename', 'charge_type']\n",
    "\n",
    "for col in cat_cols:\n",
    "    df.loc[:, col] = df.loc[:, col].astype('category')\n",
    "\n",
    "df['charge_type'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:41:55.253936Z",
     "start_time": "2019-04-23T03:41:34.301615Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9907857 entries, 0 to 9907856\n",
      "Data columns (total 6 columns):\n",
      "charge_code    6563312 non-null object\n",
      "price          8735057 non-null object\n",
      "description    8745419 non-null object\n",
      "hospital_id    9888293 non-null category\n",
      "filename       9907857 non-null category\n",
      "charge_type    9907857 non-null category\n",
      "dtypes: category(3), object(3)\n",
      "memory usage: 1.7 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage = 'deep', verbose = True, null_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice! We cut the memory usage in half!** OK, on to less obvious optimizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description and Charge Code Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:41:57.155766Z",
     "start_time": "2019-04-23T03:41:55.257186Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total missing</th>\n",
       "      <th>percent missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>charge_code</th>\n",
       "      <td>3344545</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>1172800</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>1162438</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospital_id</th>\n",
       "      <td>19564</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charge_type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             total missing  percent missing\n",
       "charge_code        3344545             0.34\n",
       "price              1172800             0.12\n",
       "description        1162438             0.12\n",
       "hospital_id          19564             0.00\n",
       "filename                 0             0.00\n",
       "charge_type              0             0.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What do our missing values look like? How sparse are these data?\n",
    "\n",
    "missing = pd.DataFrame(df.isnull().sum()).rename(columns = {0: 'total missing'})\n",
    "missing['percent missing'] = round(missing['total missing'] / len(df),2)\n",
    "missing.sort_values('total missing', ascending = False)\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like we have description text for all but 12% of the data.** Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:02.293257Z",
     "start_time": "2019-04-23T03:41:57.159940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     6563312.0\n",
       "unique    1878137.0\n",
       "top          1001.0\n",
       "freq        67966.0\n",
       "Name: charge_code, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often do charge codes exist?\n",
    "df['charge_code'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:06.897213Z",
     "start_time": "2019-04-23T03:42:02.296825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001.0         67966\n",
       "C1776          39701\n",
       "301.0          28342\n",
       "361.0          28316\n",
       "0272           20236\n",
       "320.0          19351\n",
       "27800037.0     14734\n",
       "306.0          11891\n",
       "C1713          11710\n",
       "0278           11686\n",
       "302.0          10656\n",
       "27800002       10043\n",
       "J3490           9693\n",
       "510.0           8752\n",
       "278.0           8161\n",
       "360.0           7689\n",
       "270.0           7647\n",
       "0761            7466\n",
       "250.0           7199\n",
       "0360            6598\n",
       "27800072.0      6391\n",
       "0270            5662\n",
       "63700001        5551\n",
       "272.0           5534\n",
       "0301            5509\n",
       "305.0           5191\n",
       "27200178        5028\n",
       "27200030        4821\n",
       "SCREW           4725\n",
       "272.0           4597\n",
       "               ...  \n",
       "35474618           1\n",
       "417173256.0        1\n",
       "33112860.0         1\n",
       "884214             1\n",
       "HC3010082          1\n",
       "56307630           1\n",
       "RAD01              1\n",
       "234                1\n",
       "4509203250         1\n",
       "89926              1\n",
       "351233             1\n",
       "92215274.0         1\n",
       "78931649.0         1\n",
       "52530938           1\n",
       "352625             1\n",
       "1105780            1\n",
       "573461             1\n",
       "4050041290         1\n",
       "93559466           1\n",
       "160555.0           1\n",
       "11347306           1\n",
       "4050780            1\n",
       "143984             1\n",
       "78566247.0         1\n",
       "574817             1\n",
       "155171             1\n",
       "2507629            1\n",
       "50105094           1\n",
       "120082             1\n",
       "3211468            1\n",
       "Name: charge_code, Length: 1878137, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charge_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quite a few different charge codes, with lots of different possible values.** Given that charge codes are likely a somewhat-random \"unique\" identifiers (air quotes because I'm suspicious of any large entity's data management practices until that suspicion is proven unwarranted). Nothing to see here.\n",
    "\n",
    "*OK, let's get to the meat of it, the thing that's actually most interesting (arguably): the price!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Price Column, AKA The Super Important Yet Extremely Messy Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:09.010850Z",
     "start_time": "2019-04-23T03:42:06.899530Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>      4673821\n",
       "<class 'float'>    4061236\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate out only the price rows that are non-numeric\n",
    "\n",
    "df['price'].dropna().apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So about half of the non-null prices are `str` type.** Now we need to figure out what those strings actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:10.551098Z",
     "start_time": "2019-04-23T03:42:09.013885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655360           0.0\n",
       "655361           0.0\n",
       "655362           0.0\n",
       "655363           0.0\n",
       "655364           0.0\n",
       "655365           0.0\n",
       "655366           0.0\n",
       "655367           0.0\n",
       "655368           0.0\n",
       "655369           0.0\n",
       "655370           0.0\n",
       "655371           0.0\n",
       "655372           0.0\n",
       "655373           0.0\n",
       "655374           0.0\n",
       "655375           0.0\n",
       "655376           0.0\n",
       "655377           0.0\n",
       "655378           0.0\n",
       "655379           0.0\n",
       "655380           0.0\n",
       "655381           0.0\n",
       "655382           0.0\n",
       "655383           0.0\n",
       "655384           0.0\n",
       "655385           0.0\n",
       "655386           0.0\n",
       "655387           0.0\n",
       "655388           0.0\n",
       "655389           0.0\n",
       "             ...    \n",
       "9907827     42609.08\n",
       "9907828     17784.24\n",
       "9907829      53461.5\n",
       "9907830     29037.21\n",
       "9907831     41134.92\n",
       "9907832     44698.45\n",
       "9907833     18743.77\n",
       "9907834     57341.27\n",
       "9907835     17131.68\n",
       "9907836     25042.51\n",
       "9907837    120471.76\n",
       "9907838       1393.0\n",
       "9907839     22365.63\n",
       "9907840     24346.83\n",
       "9907841     11872.24\n",
       "9907842     81348.51\n",
       "9907843     61470.69\n",
       "9907844     44314.73\n",
       "9907845     89398.38\n",
       "9907846     55705.62\n",
       "9907847     66399.91\n",
       "9907848      14493.3\n",
       "9907849     29557.59\n",
       "9907850     90269.03\n",
       "9907851     39094.68\n",
       "9907852    113699.36\n",
       "9907853     91145.59\n",
       "9907854    101386.94\n",
       "9907855       4701.0\n",
       "9907856     67846.66\n",
       "Name: price, Length: 4673821, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at only string prices\n",
    "\n",
    "df.loc[df['price'].apply(type) == str,'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T21:16:14.288910Z",
     "start_time": "2019-04-18T21:16:14.158187Z"
    }
   },
   "source": [
    "**Huh, how odd. These strings all look like regular old float values.** Why is pandas importing them as strings? Let's force the column to be float type and then we'll see how our missing values change (since we can force non-numeric strings to be `NaN`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:12.727162Z",
     "start_time": "2019-04-23T03:42:10.553476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294073"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert price to be float type, then see if missing['total_missing'] changes\n",
    "\n",
    "missing_price_initial = missing.loc['price','total missing']\n",
    "\n",
    "delta = pd.to_numeric(df['price'], errors = 'coerce').isnull().sum() - missing_price_initial\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, so we see about 300K of the 4.67M string values become `NaN` when we do this numeric conversion. That's not ideal.** Looks like we're losing a lot of data with this conversion, is there a better way? Or should we just consider this an acceptable loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:14.841804Z",
     "start_time": "2019-04-23T03:42:12.729885Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762340      5,000\n",
       "762342      1,768\n",
       "762348      8,332\n",
       "762349      2,758\n",
       "762350      1,581\n",
       "762352      2,126\n",
       "762353      1,457\n",
       "762357      2,125\n",
       "762358      1,689\n",
       "762361      4,949\n",
       "762363      2,773\n",
       "762364     13,801\n",
       "762365      7,425\n",
       "762366      8,035\n",
       "762367      7,996\n",
       "762370     32,926\n",
       "762371     19,395\n",
       "762372      2,831\n",
       "762373     10,695\n",
       "762400      1,019\n",
       "762401      1,019\n",
       "762406      1,248\n",
       "762409      1,114\n",
       "762414      4,519\n",
       "762416      4,557\n",
       "762419      6,418\n",
       "762420     27,404\n",
       "762421      8,976\n",
       "762422      1,458\n",
       "762423      6,418\n",
       "            ...  \n",
       "9897874      -   \n",
       "9897876      -   \n",
       "9897884      -   \n",
       "9897902      -   \n",
       "9897903      -   \n",
       "9897912      -   \n",
       "9897926      -   \n",
       "9897945      -   \n",
       "9897977      -   \n",
       "9897978      -   \n",
       "9898026      -   \n",
       "9898027      -   \n",
       "9898034      -   \n",
       "9898037      -   \n",
       "9898038      -   \n",
       "9898041      -   \n",
       "9898042      -   \n",
       "9898052      -   \n",
       "9898057      -   \n",
       "9898058      -   \n",
       "9898070      -   \n",
       "9898081      -   \n",
       "9898086      -   \n",
       "9898096      -   \n",
       "9898304      -   \n",
       "9898308      -   \n",
       "9898310      -   \n",
       "9898311      -   \n",
       "9898324      -   \n",
       "9898325      -   \n",
       "Name: price, Length: 1466873, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to filter so we can see a sample of the ~300K true non-float strings in price\n",
    "# so we can get an idea as to how to deal with them?\n",
    "\n",
    "# Show only the prices that are null after numeric conversion\n",
    "df.loc[pd.to_numeric(df['price'], errors = 'coerce').isnull(), 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ah, I see the issues here! We've got commas making numbers look like they're not numbers, and this weird ' -   ' string that must be a certain hospitals equivalent of `null`.** Let's correct for these!\n",
    "\n",
    "The `null` string is one space, following by a hyphen and three spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:15.331561Z",
     "start_time": "2019-04-23T03:42:14.845489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set ' -   ' to proper NaN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df['price'].replace(' -   ', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:26.362269Z",
     "start_time": "2019-04-23T03:42:15.333338Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762340                      5,000\n",
       "762342                      1,768\n",
       "762348                      8,332\n",
       "762349                      2,758\n",
       "762350                      1,581\n",
       "762352                      2,126\n",
       "762353                      1,457\n",
       "762357                      2,125\n",
       "762358                      1,689\n",
       "762361                      4,949\n",
       "762363                      2,773\n",
       "762364                     13,801\n",
       "762365                      7,425\n",
       "762366                      8,035\n",
       "762367                      7,996\n",
       "762370                     32,926\n",
       "762371                     19,395\n",
       "762372                      2,831\n",
       "762373                     10,695\n",
       "762400                      1,019\n",
       "762401                      1,019\n",
       "762406                      1,248\n",
       "762409                      1,114\n",
       "762414                      4,519\n",
       "762416                      4,557\n",
       "762419                      6,418\n",
       "762420                     27,404\n",
       "762421                      8,976\n",
       "762422                      1,458\n",
       "762423                      6,418\n",
       "                    ...          \n",
       "9736567     $1,104.00 - $1,421.00\n",
       "9736572     $1,479.00 - $1,904.00\n",
       "9736573       $900.00 - $1,159.00\n",
       "9736574     $1,146.00 - $1,476.00\n",
       "9736600     $1,700.00 - $2,189.00\n",
       "9736601     $1,824.00 - $2,348.00\n",
       "9736602     $2,201.00 - $2,834.00\n",
       "9736603     $1,498.00 - $1,929.00\n",
       "9737129       $599.00 - $1,112.00\n",
       "9737177       $881.00 - $1,320.00\n",
       "9737726        $5.71 - $10,500.00\n",
       "9737769       $990.00 - $1,485.00\n",
       "9737889         $1.00 - $9,600.00\n",
       "9737943       $812.00 - $1,506.00\n",
       "9737944       $696.00 - $1,292.00\n",
       "9737945     $1,461.00 - $2,191.00\n",
       "9737946       $743.00 - $1,114.00\n",
       "9737956       $11.64 - $11,700.00\n",
       "9737957     $1,414.00 - $2,626.00\n",
       "9737964     $1,274.00 - $2,365.00\n",
       "9737994       $743.00 - $1,114.00\n",
       "9738010       $700.00 - $1,051.00\n",
       "9738125       $681.00 - $1,264.00\n",
       "9738329       $827.00 - $1,537.00\n",
       "9738463     $2,912.00 - $3,490.24\n",
       "9738465      $208.00 - $38,640.93\n",
       "9738528      $416.00 - $15,618.72\n",
       "9738585    $2,325.44 - $23,296.00\n",
       "9738655    $2,496.00 - $14,231.36\n",
       "9739988                 $2,416.41\n",
       "Name: price, Length: 39028, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the values that contain commas?\n",
    "\n",
    "df.loc[df['price'].str.contains(r'\\d,\\d', na = False), 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oh goodie, there are *ranges* of dollar amounts that we have to worry about!** Fantastic, just fantastic. These hospitals sure don't like making this easy, do they?\n",
    "\n",
    "Here's the plan:\n",
    "1. Strip dollar signs from all of these strings. They're superfluous and just make things complicated\n",
    "2. Split strings on the string ' - '\n",
    "3. Remove commas and cast results as float values\n",
    "    * Any rows with `NaN` for the second split column are just regular comma-in-the-thousands-place numbers. Leave them alone for the moment\n",
    "    * Any with non-null values in the second column: take the average of the two columns and overwrite the first column with the average. This midpoint value will be considered the useful estimate of the cost\n",
    "4. Push the first column values (now overwritten in the case of ranges of values to be the midpoints) back into their places in the `price` column and continue hunting for edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:27.661165Z",
     "start_time": "2019-04-23T03:42:26.365429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace effectively null prices with np.nan\n",
    "\n",
    "df.loc[df['price'] == '-', 'price'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:42:27.689683Z",
     "start_time": "2019-04-23T03:42:27.662787Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_silly_chars(row):\n",
    "    '''\n",
    "    It is assumed that this will be used as part of a Series.apply() call.\n",
    "    \n",
    "    Takes in an element of a pandas Series that should be a string representation \n",
    "    of a price or a range of prices and spits back the string without any thousands\n",
    "    separators (e.g. commas) or dollar signs. If it detects that there's a range of\n",
    "    prices being provided (e.g. $1.00 - $2.00), it returns the midpoint of the range.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row: str. The string representation of a price or range of prices.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float. The price (or midpoint price if a range was given in row)\n",
    "    '''\n",
    "    \n",
    "    # Replace '$', ',', and '-' with empty strings, splitting on the hyphen\n",
    "    price_strings = row.replace('$','').replace(',','').split('-')\n",
    "    \n",
    "    \n",
    "    # Strip leading and trailing whitespace from list of split strings\n",
    "    # and take the average of the different prices from the original range,\n",
    "    # returning it as a float value\n",
    "    \n",
    "    # If ValueError raised, assume there were invalid characters and\n",
    "    # set to np.nan\n",
    "    \n",
    "    try:\n",
    "        return pd.Series([float(i.strip()) for i in price_strings]).mean()\n",
    "    \n",
    "    except ValueError: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:48:43.059767Z",
     "start_time": "2019-04-23T03:48:02.121339Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When only digits + commas + $, convert to only digits\n",
    "# Also take average if multiple value range provided in string\n",
    "ix = df['price'].str.contains(r'\\d,\\d|\\$\\d', na=False, case=False)\n",
    "df.loc[ix, 'price'] = df.loc[ix,'price'].apply(remove_silly_chars)                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, now that we've cleaned up a lot of those strings so that they can be proper floats, how many bad strings are we left with?** When we ran this check prior to the `$` and `,` cleaning process, we had about 300K records coming up as null after the numeric conversion that weren't null previously. What does it look like now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:49:24.500175Z",
     "start_time": "2019-04-23T03:49:22.211698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206898"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert price to be float type, then see if missing['total_missing'] changes\n",
    "\n",
    "missing_price_initial = missing.loc['price','total missing']\n",
    "\n",
    "delta = pd.to_numeric(df['price'], errors = 'coerce').isnull().sum() - missing_price_initial\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIGH. Now we're done to 200K, but that's not awesome.** At this point, I'm fine with it. We'll lose about 2.1% of these that may not appropriately be null (although most probably should be), but anything else I've tried to pare it down further actually makes things worse, so I say we're good now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:49:41.864712Z",
     "start_time": "2019-04-23T03:49:39.057760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379698"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coerce any obvious float strings to floats\n",
    "df.loc[:, 'price'] = pd.to_numeric(df['price'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:49:57.572148Z",
     "start_time": "2019-04-23T03:49:42.649562Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9907857 entries, 0 to 9907856\n",
      "Data columns (total 6 columns):\n",
      "charge_code    6563312 non-null object\n",
      "price          8528159 non-null float64\n",
      "description    8745419 non-null object\n",
      "hospital_id    9888293 non-null category\n",
      "filename       9907857 non-null category\n",
      "charge_type    9907857 non-null category\n",
      "dtypes: category(3), float64(1), object(2)\n",
      "memory usage: 1.4 GB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage = 'deep', verbose = True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-23T03:52:08.396900Z",
     "start_time": "2019-04-23T03:52:07.771809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.528159e+06\n",
       "mean     8.959729e+06\n",
       "std      1.928350e+08\n",
       "min     -1.298600e+04\n",
       "25%      1.590300e+02\n",
       "50%      9.020000e+02\n",
       "75%      4.449000e+03\n",
       "max      1.000000e+10\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Welp, that's the best we can do!** Now I just need to write this up as a standalone `import_data` script for easy loading in the analysis stage.\n",
    "\n",
    "NOTE: there are still some messed up prices in this, as some items that came through as float are actually charge codes and their charge codes are actually prices. Best we can do is deal with the outliers and hope the rest are reasonably accurate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HospitalPricing]",
   "language": "python",
   "name": "conda-env-HospitalPricing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "140px",
    "left": "786px",
    "right": "20px",
    "top": "134px",
    "width": "420px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
